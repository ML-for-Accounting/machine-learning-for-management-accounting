{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing for conventional text analysis methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code includes the example of Section 2.2 in the article \"Machine learning in management accounting research: Literature review and pathways for the future\". The article is forthcoming in European Accounting Review, but the working paper version can be downloaded from https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3822650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, this example load everything into memory at each step. This is not efficient and an iterative approach should be used with larger textual datasets. The example data consists of 100 10-X filings of manufacturing companies from the year 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information texts are color-coded to make it easier to find parts relevant for the discussion of the accompanying article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Yellow is used for parts of the code which are irrelevant and perform, for example, pre-processing operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "Blue is used for the relevant parts of the code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import gensim_lda_library as gl\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Part 1: Transform pdf:s to pure text\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using convert_pdfminer() -function from the specific libary to convert pdf files to pure text and insterting text as long strings to raw_text list. The details of the function are explained in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './example22_data/'\n",
    "files = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = []\n",
    "for file in files:\n",
    "    temp1 = open(data_path+file,'r').read()\n",
    "    raw_text.append(temp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"atements of Operations   \\n           2   \\n\\nCondensed Consolidated Statements of Changes in Shareholders  Deficiency   \\n           3   \\n\\nCondensed Consolidated Statements of Cash Flows   \\n           4   \\n\\nNotes to Condensed Consolidated Financial Statements     \\n           5   \\n \\n      Item\\n    2.     Management's Discussion and Analysis of Financial Condition and Results of Operations   \\n         \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[0][3000:3400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Part 2: First cleaning steps\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define the stopwords list (NLTK library). The list can be extended easily with \"extend\"-method.Here extended with the words'https','doi','org'\n",
    "* Simple_preprocess() -function of Gensim can be used basic cleaning steps. With default settings, it will remove numbers, words shorter than 2 characters and words longer than 15 characters \n",
    "* Use the stopwords list of the NLTK library to remove the stopwords from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend(['https','doi','org'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_cleaned = []\n",
    "for item in raw_text:\n",
    "    tokens = gensim.utils.simple_preprocess(item)\n",
    "    docs_cleaned.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unaudited', 'financial', 'statements', 'table', 'of', 'contents']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_cleaned[0][500:506]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_nostops = []\n",
    "for item in docs_cleaned:\n",
    "    red_tokens = [word for word in item if word not in stop_words]\n",
    "    docs_nostops.append(red_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['common', 'stock', 'shares', 'note', 'going', 'concern']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_nostops[0][500:506]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Part 3: Use the Spacy deep learning language model to remove specific parts-of-speech\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define which PoS will be saved\n",
    "* Load the deep learning model. For this application, we do not need the parser and named-entity-reconginition modules\n",
    "* Go through the texts and keep only nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "allowed_postags=['NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_lemmas = []\n",
    "for red_tokens in docs_nostops:\n",
    "    doc = nlp(\" \".join(red_tokens))\n",
    "    docs_lemmas.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'service', 'customer', 'time', 'amount', 'contract']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_lemmas[0][500:506]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Part 4: Create bigrams/trigrams\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(docs_lemmas,threshold = 25, min_count=2)\n",
    "#trigram = gensim.models.Phrases(bigram[docs_lemmas], threshold=25, min_count=2)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bigrams = [bigram_mod[doc] for doc in docs_lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['credit_worthiness', 'trend', 'account', 'effort', 'collection', 'company']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_bigrams[0][540:546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_trigrams = [trigram_mod[doc] for doc in docs_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Create dictionary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dictionary is created from the cleaned texts in docs_bigrams\n",
    "* Abnormal words are removed from the dictionary\n",
    "    - Words that are present in just 2 or less texts\n",
    "    - Words that are present in more than 70 % of the texts\n",
    "    - 50000 most common words are kept\n",
    "* The dictionary is used to create a bag-of-words representation of the words, which is saved to the corpus-list. The list contains counts for each dictionary word.\n",
    "* Word \"allowance\" 7 times in the first article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(docs_bigrams)\n",
    "id2word.filter_extremes(no_below=2, no_above=0.7, keep_n=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in docs_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 2), (3, 2), (4, 1), (5, 7)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allowance'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
