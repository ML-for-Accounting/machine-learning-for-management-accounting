{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "possible-fleece",
   "metadata": {},
   "source": [
    "### Word2vec example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-sunset",
   "metadata": {},
   "source": [
    "This code includes the example of Section 3.3 in the article \"Machine learning in management accounting research: Literature review and pathways for the future\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-animal",
   "metadata": {},
   "source": [
    "Note! This code does not use the language model of the paper used to infer similar phrases for \"restructuring\" and \"growth_strategy\" due to the model being several gigabytes in size. For demonstration purposes, this code includes a simpler model that is trained using text8 dataset in Gensim. It contains textual data from Wikipedia. However, this code includes guidelines how to construct a similar language model using 10-K filings that was used in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-discussion",
   "metadata": {},
   "source": [
    "Different tasks are separated to different parts below. Of course in practical applications they can be done in one pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-portsmouth",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Yellow is used for parts of the code which are irrelevant and perform, for example, pre-processing operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-domestic",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "Blue is used for the relevant parts of the code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "homeless-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-tracker",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-entrepreneur",
   "metadata": {},
   "source": [
    "### Part 1: Replacing named entities with tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defensive-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'your_source_dir_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acceptable-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = 'your_dest_dir_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-haven",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Load the Spacy model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-zambia",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "We add \"merge entities\" module to the pipeline to connect entities that consist of several words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "illegal-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "referenced-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('merge_entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-payment",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "The algorithm below can be used to transform files in \"source_dir\" to such that named entities have been replaced with \"ner_(entity_type)\" tags. np.setdiff1d is used to collect the remaining files that have not yet been processed. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = os.listdir(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "headed-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "files2 = os.listdir(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noticed-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_files = np.setdiff1d(files1,files2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-penetration",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Algorithm for replacing named entities with a tag ner_(type of named entity)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cubic-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in remaining_files:\n",
    "    raw = open(os.path.join(source_dir, fname)).read()\n",
    "    raw = raw[500:1000000].lower()\n",
    "    raw = \" \".join(gensim.utils.simple_preprocess(raw))\n",
    "    doc=nlp(raw,disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "    open(os.path.join(dest_dir + fname),mode='w').write(' '.join([t.text if not t.ent_type_ else 'ner_' + t.ent_type_ for t in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-cartoon",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-instrumentation",
   "metadata": {},
   "source": [
    "Below is an example output what the algorithm above produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "independent-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = './example22_data/20180814_707549.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "legislative-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open(example_file).read()\n",
    "raw = raw[500:1000000].lower()\n",
    "raw = \" \".join(gensim.utils.simple_preprocess(raw))\n",
    "doc=nlp(raw,disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "excellent-discrimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' we make with the ner_ORG are available on our website free of charge as soon as reasonably practical after we file them with or furnish them to the ner_ORG and are also available online at the ner_ORG website at www sec gov any materials we file with the ner_ORG may also be read and copied at the ner_ORG public reference room at street ne ner_GPE to obtain information on the operation of the publ'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([t.text if not t.ent_type_ else 'ner_' + t.ent_type_ for t in doc])[9600:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-macro",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-owner",
   "metadata": {},
   "source": [
    "### Part 2: Create noun chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acoustic-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'your_source_dir_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greatest-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = 'your_dest_dir_here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "duplicate-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-intention",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Add module to the pipeline that creates noun chunks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "protected-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('merge_noun_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-shopper",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "The algorithm below can be used to transform files in \"source_dir\" to such that words related to nouns are connected with \"_\". np.setdiff1d is used to collect the remaining files that have not yet been processed. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "russian-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = os.listdir(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reliable-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "files2 = os.listdir(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "internal-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dietary-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_files = np.setdiff1d(files1,files2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-firmware",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Combine the words of noun chunks with '_'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "matched-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in remaining_files:\n",
    "    raw = open(os.path.join(source_dir, fname)).read()\n",
    "    doc=nlp(raw,disable=[\"lemmatizer\",\"ner\"])\n",
    "    open(os.path.join(dest_dir + fname),mode='w').write(' '.join([t.text.replace(' ','_') for t in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-aurora",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-grass",
   "metadata": {},
   "source": [
    "Below is an example output what the algorithm above produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "reliable-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_file = './example22_data/20180814_707549.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "numerical-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open(example_file).read()\n",
    "raw = \" \".join(gensim.utils.simple_preprocess(raw))\n",
    "doc=nlp(raw,disable=[\"lemmatizer\",\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "english-basket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ject to the_safe_harbor_provisions created by the_private_securities_litigation_reform_act of certain but not all of the_forward_looking_statements in this_report are specifically identified as forwar'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([t.text.replace(' ','_') for t in doc])[5000:5200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-background",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-symposium",
   "metadata": {},
   "source": [
    "### Part 3: Identify similar phrases using a word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-saver",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Use a word2vec model, trained with 10-Ks, to infer most similar words to specified keywords.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-sample",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Main routine to create the word2vec model. NOTE! For this code to work, you need a collection of texts. In the article 180 000 10-K filings were used to train the word2vec model. This algorithm creates the model iteratively without loading the whole textual data to the memory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "necessary-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            raw = open(os.path.join(self.dirname, fname)).read()\n",
    "            raw = raw.lower()\n",
    "            yield gensim.utils.simple_preprocess(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painted-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_10K = MySentences(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(docs_10K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-heath",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-formula",
   "metadata": {},
   "source": [
    "NOTE! Below is an example how the model above can be used to search similar phrases to the selected keywords. However, the example below uses the text8 dataset included in Gensim, not the model that was used in the paper. The text8 dataset contains text from Wikipedia, so the closest words are different than from the model trained using 10-Ks. (Again a justification that we should use domain-specific textual data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-enclosure",
   "metadata": {},
   "source": [
    "NOTE! The text8 dataset does not contain ner-tags or noun chunks, and therefore, there are no phrases in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "rotary-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "statutory-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gensim.downloader.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "israeli-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-polyester",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "The 4 closest words the keywpords 'restructuring'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "baking-album",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('liberalization', 0.8058362603187561),\n",
       " ('modernization', 0.7889036536216736),\n",
       " ('privatization', 0.754657506942749),\n",
       " ('stabilization', 0.7462006211280823)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['restructuring'],topn=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-analyst",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Calculate the centroid vector from the word vectors representing words 'restructuring','liberalization','modernization','privatization','stabilization'. At this point, it is possible to finetune the seed words by adding your own words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "selected-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['restructuring','liberalization','modernization','privatization','stabilization']\n",
    "restr_centroid = np.zeros(100)\n",
    "for word in word_list:\n",
    "    restr_centroid = np.add(restr_centroid, model.wv[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "reverse-marsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "restr_centroid = restr_centroid/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-bailey",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Collect the 100 word vectors that are cloesest to the centroid\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "relevant-creator",
   "metadata": {},
   "outputs": [],
   "source": [
    "restr_keywords = [word for (word,_) in model.wv.most_similar(positive=restr_centroid,topn=100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "precise-consensus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restructuring',\n",
       " 'liberalization',\n",
       " 'privatization',\n",
       " 'modernization',\n",
       " 'privatisation',\n",
       " 'financing',\n",
       " 'deregulation',\n",
       " 'stabilization',\n",
       " 'industrialization',\n",
       " 'decentralization',\n",
       " 'macroeconomic',\n",
       " 'banking',\n",
       " 'austerity',\n",
       " 'liberalized',\n",
       " 'policies',\n",
       " 'perestroika',\n",
       " 'nationalisation',\n",
       " 'privatizations',\n",
       " 'infrastructure',\n",
       " 'reforms',\n",
       " 'consolidation',\n",
       " 'initiatives',\n",
       " 'deficits',\n",
       " 'instability',\n",
       " 'mismanagement',\n",
       " 'welfare',\n",
       " 'nationalization',\n",
       " 'dismantling',\n",
       " 'downturn',\n",
       " 'investment',\n",
       " 'democratization',\n",
       " 'collectivization',\n",
       " 'fiscal',\n",
       " 'subsidies',\n",
       " 'investments',\n",
       " 'overhaul',\n",
       " 'financial',\n",
       " 'enterprises',\n",
       " 'diversification',\n",
       " 'sponsorship',\n",
       " 'liberalisation',\n",
       " 'finances',\n",
       " 'multilateral',\n",
       " 'monetary',\n",
       " 'tariff',\n",
       " 'lobbying',\n",
       " 'reform',\n",
       " 'eradication',\n",
       " 'glasnost',\n",
       " 'adjustment',\n",
       " 'employment',\n",
       " 'economic',\n",
       " 'bureaucratic',\n",
       " 'procurement',\n",
       " 'monopolies',\n",
       " 'taxation',\n",
       " 'policy',\n",
       " 'securities',\n",
       " 'crises',\n",
       " 'outsourcing',\n",
       " 'centralization',\n",
       " 'administration',\n",
       " 'incentives',\n",
       " 'surpluses',\n",
       " 'cutbacks',\n",
       " 'interventions',\n",
       " 'tariffs',\n",
       " 'priorities',\n",
       " 'initiative',\n",
       " 'expenditure',\n",
       " 'bargaining',\n",
       " 'pension',\n",
       " 'usaid',\n",
       " 'exploitation',\n",
       " 'sector',\n",
       " 'productivity',\n",
       " 'unionism',\n",
       " 'volatility',\n",
       " 'easing',\n",
       " 'loans',\n",
       " 'profitability',\n",
       " 'finance',\n",
       " 'regulation',\n",
       " 'infrastructures',\n",
       " 'deficit',\n",
       " 'workforce',\n",
       " 'keynesianism',\n",
       " 'burdens',\n",
       " 'wholesale',\n",
       " 'markets',\n",
       " 'recession',\n",
       " 'facilitating',\n",
       " 'funding',\n",
       " 'corruption',\n",
       " 'acquisitions',\n",
       " 'reductions',\n",
       " 'recovery',\n",
       " 'shortages',\n",
       " 'industrialisation',\n",
       " 'economy']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restr_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-sapphire",
   "metadata": {},
   "source": [
    "### Part 4: Build the measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-coral",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "When the phrases are identified, the code below canbe used to calculate the occurence of words in 10-Ks\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "adjusted-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_10K = MySentences(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "isolated-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = []\n",
    "for doc in docs_10K:\n",
    "    temp_sum = 0\n",
    "    for word in restr_keywords:\n",
    "        temp_sum+=doc.count(word)\n",
    "    count_list.append(temp_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
