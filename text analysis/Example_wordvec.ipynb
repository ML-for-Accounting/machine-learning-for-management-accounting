{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parental-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import Cython\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-maria",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Yellow is used for parts of the code which are irrelevant and perform, for example, pre-processing operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-sessions",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert-info\">\n",
    "Blue is used for the relevant parts of the code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-sustainability",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "female-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'D:/Data/Large_10K_corpus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "friendly-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = 'D:/temp3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-midnight",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Load the Spacy model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "olympic-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-exhibit",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "We add \"merge entities\" module to the pipeline to connect entities that consist of several words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-formation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('merge_entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-cursor",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Collect the remaining files that have not yet been processed\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "armed-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = os.listdir(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cooked-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "files2 = os.listdir(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excited-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_files = np.setdiff1d(files1,files2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-costa",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Algorithm for replacing named entities with a tag ner_(type of named entity)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hydraulic-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in remaining_files:\n",
    "    raw = open(os.path.join(source_dir, fname)).read().split('</Header>')[1]\n",
    "    raw = raw[500:1000000].lower()\n",
    "    raw = \" \".join(gensim.utils.simple_preprocess(raw))\n",
    "    doc=nlp(raw,disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "    open(os.path.join(dest_dir + fname),mode='w').write(' '.join([t.text if not t.ent_type_ else 'ner_' + t.ent_type_ for t in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-electronics",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "weighted-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simplified-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = 'D:/temp3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stylish-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = 'D:/NOUN_CHUNKS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outstanding-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-venezuela",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Add module to the pipeline that creates noun chunks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "engaging-cloud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function spacy.pipeline.functions.merge_noun_chunks(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('merge_noun_chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worse-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = os.listdir(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "productive-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "files2 = os.listdir(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hearing-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "supreme-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_files = np.setdiff1d(files1,files2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-yeast",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Combine the words of noun chunks with '_'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "promotional-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in remaining_files:\n",
    "    raw = open(os.path.join(source_dir, fname)).read()\n",
    "    doc=nlp(raw,disable=[\"lemmatizer\",\"ner\"])\n",
    "    open(os.path.join(dest_dir + fname),mode='w').write(' '.join([t.text.replace(' ','_') for t in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-kingdom",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-stanley",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Use a word2vec model, trained with 10-Ks, to infer most similar words to specified keywords.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-level",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Main routine to create the word2vec model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "extended-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            raw = open(os.path.join(self.dirname, fname)).read()\n",
    "            raw = raw.lower()\n",
    "            raw = raw.replace('non-gaap','nongaap')\n",
    "            raw = raw.replace('non gaap','nongaap')\n",
    "            raw = raw.replace('pro-forma','proforma')\n",
    "            raw = raw.replace('pro forma','proforma')\n",
    "            yield gensim.utils.simple_preprocess(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pretty-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_10K = MySentences(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "animated-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(docs_10K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-guidance",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "The 50 closest words the keywpords 'sustainability' and 'climate_change'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.wv.most_similar(positive=['sustainability','climate_change'],topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-congress",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Calculate the centroid vector from the word vectors representing words 'restructuring','rationalization','downsizing','resizing','plant_closures'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "executive-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['restructuring','rationalization','downsizing','resizing','plant_closures']\n",
    "restr_centroid = np.zeros(100)\n",
    "for word in word_list:\n",
    "    restr_centroid = np.add(restr_centroid, new_model.wv[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "peripheral-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "restr_centroid = restr_centroid/5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-assumption",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Collect the 100 word vectors that are cloesest to the centroid\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "identified-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "restr_keywords = [word for (word,_) in new_model.wv.most_similar(positive=restr_centroid,topn=100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-firmware",
   "metadata": {},
   "source": [
    "<div class=\"alert-info\">\n",
    "Calculate the occurence of words in 10-Ks\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "secure-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_10K = MySentences(source_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "determined-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = []\n",
    "for doc in docs_10K:\n",
    "    temp_sum = 0\n",
    "    for word in restr_keywords:\n",
    "        temp_sum+=doc.count(word)\n",
    "    count_list.append(temp_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
